{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9cd7d13-189d-48c9-9744-7a99e0f0a7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d461d08-f13a-40f6-bf67-fd5418e91276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# logger.debug(\"Teste de log DEBUG\")\n",
    "# logger.info(\"This is an info message.\")\n",
    "# logger.warning(\"This is a warning message.\")\n",
    "# logger.error(\"This is an error message.\")\n",
    "# logger.critical(\"This is a critical message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a0f055-7e39-4933-a0e1-f35c612ec175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    logger.info(\"1/4 - Buscando segredos do Databricks Secrets...\")\n",
    "    scope_name = \"guilherme_ferreira_checkpoint2_lh\"\n",
    "    user = dbutils.secrets.get(scope=scope_name, key=\"db_user\")\n",
    "    password = dbutils.secrets.get(scope=scope_name, key=\"db_pass\")\n",
    "    host = dbutils.secrets.get(scope=scope_name, key=\"db_host\")\n",
    "    port = dbutils.secrets.get(scope=scope_name, key=\"db_port\")\n",
    "    logger.info(\"Segredos obtidos com sucesso.\")\n",
    "\n",
    "    logger.info(\"\\n2/4 - Configurando a conexão JDBC...\")\n",
    "    jdbc_url = f\"jdbc:sqlserver://{host}:{port};databaseName=AdventureWorks\"\n",
    "    connection_props = {\n",
    "      \"user\": user,\n",
    "      \"password\": password,\n",
    "      \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "      \"encrypt\": \"true\",\n",
    "      \"trustServerCertificate\": \"true\"\n",
    "    }\n",
    "    logger.info(\"Configuração da conexão finalizada.\")\n",
    "\n",
    "    spark.sql(\"USE CATALOG ted_dev\")\n",
    "    spark.sql(\"USE SCHEMA dev_guilherme_sobrinho\")\n",
    "    logger.info(\"Configuração de CATALOG e SCHEMA finalizada.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"\\nOCORREU UM ERRO DURANTE A EXECUÇÃO:\")\n",
    "    logger.error(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d592bd-a2f8-4b11-a7b2-944149417a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para ingestão com schema dinâmico do banco e sanitização de colunas\n",
    "\n",
    "\n",
    "def sanitizar_colunas(df):\n",
    "    colunas_corrigidas = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "    return df.select(*colunas_corrigidas)\n",
    "\n",
    "def ingerir_tabela_completa(nome_tabela_completo):\n",
    "    nome_logico = nome_tabela_completo.split(\".\")[-1].lower()\n",
    "    nome_delta = f\"raw_database_{nome_logico}\"\n",
    "    logger.info(f\"Ingerindo: {nome_tabela_completo} como {nome_delta}\")\n",
    "    query = f\"(SELECT * FROM {nome_tabela_completo}) AS temp\"\n",
    "\n",
    "    # Lê dados do banco\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_props)\n",
    "\n",
    "    df = sanitizar_colunas(df)\n",
    "\n",
    "    # Grava a tabela no Delta Lake com o prefixo raw_database_<nome-da-tabela>\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_delta)\n",
    "\n",
    "    logger.info(f\"{nome_delta} salva com sucesso.\")\n",
    "\n",
    "\n",
    "# Lê o catálogo de tabelas disponíveis no banco\n",
    "df_tabelas = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"(SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE') AS temp\",\n",
    "    properties=connection_props\n",
    ")\n",
    "\n",
    "# Gera a coluna com o nome completo da tabela (schema.table)\n",
    "df_tabelas_completo = (\n",
    "    df_tabelas\n",
    "    .withColumn(\"nome_completo\", expr(\"concat(TABLE_SCHEMA, '.', TABLE_NAME)\"))\n",
    "    .selectExpr(\"nome_completo as tabela\")\n",
    ")\n",
    "\n",
    "# Limita a 1000 tabelas para evitar sobrecarga no driver, o total de tabelas ingeridas é cerca de 70.\n",
    "tabelas_para_ingestao = df_tabelas_completo.limit(1000).collect()\n",
    "logger.info(f\"Total de tabelas para ingestão: {len(tabelas_para_ingestao)}\")\n",
    "\n",
    "# Loop para ingerir as tabelas usando a função definida acima\n",
    "for row in tabelas_para_ingestao:\n",
    "    tabela_nome = row[\"tabela\"]\n",
    "    try:\n",
    "        ingerir_tabela_completa(tabela_nome)\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Erro ao ingerir {tabela_nome}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f217f5fd-0fd8-47e1-9a48-e39f5db5f5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retry configurável por timeout\n",
    "MAX_RETRIES = 3\n",
    "LIMITS_REDUZIDOS = [100, 50, 20]  # Tentativas progressivas com limites menores\n",
    "\n",
    "def ingerir_api(nome_tabela_logico, endpoint, limit=100):\n",
    "    \"\"\"\n",
    "    Função para ingerir dados paginados da API e salvar no Delta Lake.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n Iniciando ingestão da API: {endpoint} → raw_api_{nome_tabela_logico}\")\n",
    "    url_base = f\"http://18.209.218.63:8080/{endpoint}\"\n",
    "    api_user = dbutils.secrets.get(scope=\"guilherme_ferreira_checkpoint2_lh\", key=\"api_user\")\n",
    "    api_password = dbutils.secrets.get(scope=\"guilherme_ferreira_checkpoint2_lh\", key=\"api_password\")\n",
    "    auth = (api_user, api_password)\n",
    "    total_registros = 0\n",
    "    todos_dados = []\n",
    "\n",
    "    for tentativa, limit_atual in enumerate(LIMITS_REDUZIDOS, start=1):\n",
    "        logger.info(f\"\\n Tentativa {tentativa} com limit={limit_atual}\")\n",
    "        offset = 0\n",
    "        todos_dados.clear()\n",
    "        total_registros = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                url = f\"{url_base}?offset={offset}&limit={limit_atual}\"\n",
    "                try:\n",
    "                    res = requests.get(url, auth=auth, timeout=10)\n",
    "                    logger.info(f\"Status da resposta: {res.status_code}, conteúdo: {len(res.content)} bytes\")\n",
    "                except requests.exceptions.Timeout:\n",
    "                    logger.error(f\"Timeout ao acessar {url}\")\n",
    "                    break \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logger.error(f\"Erro na requisição: {e}\")\n",
    "                    break\n",
    "\n",
    "                if res.status_code != 200:\n",
    "                    logger.info(f\"API respondeu com status {res.status_code}: {res.text}\")\n",
    "                    break\n",
    "\n",
    "                json_data = res.json()\n",
    "                dados = json_data.get(\"data\", [])\n",
    "\n",
    "                if not dados:\n",
    "                    logger.warning(\"Nenhum dado retornado, encerrando a paginação.\")\n",
    "                    break\n",
    "\n",
    "                todos_dados.extend(dados)\n",
    "                total_registros += len(dados)\n",
    "                logger.info(f\"Recebidos {len(dados)} registros, total acumulado: {total_registros}\")\n",
    "\n",
    "                offset += limit_atual\n",
    "                total_api = json_data.get(\"total\", None)\n",
    "                if total_api is not None and offset >= total_api:\n",
    "                    break\n",
    "\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            if total_registros > 0:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Erro ao processar tentativa {tentativa} do endpoint {endpoint}: {e}\")\n",
    "\n",
    "    if total_registros == 0:\n",
    "        logger.warning(\"Nenhum dado foi obtido da API após todas as tentativas.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.createDataFrame(todos_dados)\n",
    "\n",
    "        # df = spark.read.json(spark.sparkContext.parallelize([json.dumps(todos_dados)]))\n",
    "        # df = df.selectExpr(\"explode(value) as row\").select(\"row.*\")\n",
    "\n",
    "        nome_delta = f\"ted_dev.dev_guilherme_sobrinho.raw_api_{nome_tabela_logico}\"\n",
    "        spark.sql(\"USE CATALOG ted_dev\")\n",
    "        spark.sql(\"USE SCHEMA dev_guilherme_sobrinho\")\n",
    "        logger.info(\"Configuração de CATALOG e SCHEMA finalizada.\")\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_delta)\n",
    "        logger.info(f\" {nome_delta} salva com sucesso com {total_registros} registros.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Erro ao salvar dados no Delta Lake para {endpoint}: {e}\")\n",
    "\n",
    "    logger.info(f\" Finalizado endpoint {endpoint} com {total_registros} registros.\")\n",
    "\n",
    "\n",
    "def ingerir_varios_endpoints(endpoints, max_workers=1):\n",
    "    \"\"\"\n",
    "    Executa ingestão paralela com número controlado de workers para vários endpoints.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\n Iniciando ingestão paralela de {len(endpoints)} endpoints com max_workers={max_workers}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(ingerir_api, nome, endpoint): nome for nome, endpoint in endpoints\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            nome = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logger.error(f\" Erro na tarefa {nome}: {e}\")\n",
    "\n",
    "    logger.info(\"Ingestão paralela finalizada.\")\n",
    "\n",
    "\n",
    "# Lista de endpoints (com nomes lógicos e nomes reais)\n",
    "endpoints = [\n",
    "    (\"salesorderdetail\", \"SalesOrderDetail\"),\n",
    "    (\"salesorderheader\", \"SalesOrderHeader\"),\n",
    "    (\"purchaseorderdetail\", \"PurchaseOrderDetail\"),\n",
    "    (\"purchaseorderheader\", \"PurchaseOrderHeader\"),\n",
    "]\n",
    "\n",
    "# Executar com paralelismo ajustável (pode aumentar conforme o comportamento da API)\n",
    "ingerir_varios_endpoints(endpoints, max_workers=2)  # cuidado pra não aumentar demais e causar timeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdba951-7e14-4c1c-80a7-b2620e448429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Teste se a API está funcionando\n",
    "# url = \"http://18.209.218.63:8080/SalesOrderHeader?offset=0&limit=10\"\n",
    "# api_user = dbutils.secrets.get(scope=\"guilherme_ferreira_checkpoint2_lh\", key=\"api_user\")\n",
    "# api_password = dbutils.secrets.get(scope=\"guilherme_ferreira_checkpoint2_lh\", key=\"api_password\")\n",
    "# res = requests.get(url, auth=(api_user, api_password))\n",
    "# logger.info(res.status_code)\n",
    "# logger.info(res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623ea78b-a7da-4345-9cf6-39a0ce4d5c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trecho para excluir tudo que eu ingeri\n",
    "\n",
    "\n",
    "# schema = \"dev_guilherme_sobrinho\"\n",
    "\n",
    "# # Listar todas as tabelas do schema\n",
    "# tabelas = spark.sql(f\"SHOW TABLES IN {schema}\").filter(\"isTemporary = false\").select(\"tableName\").collect()\n",
    "\n",
    "# for t in tabelas:\n",
    "#     nome_tabela = t[\"tableName\"]\n",
    "#     full_table_name = f\"{schema}.{nome_tabela}\"\n",
    "#     try:\n",
    "#         logger.info(f\"Apagando tabela {full_table_name} ...\")\n",
    "#         spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "#         logger.info(f\"Tabela {full_table_name} apagada.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Erro ao apagar {full_table_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks-database-ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
