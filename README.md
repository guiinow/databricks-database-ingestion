
# Projeto: Ingest√£o de Dados - SQL Server e API RESTful

## üìå Descri√ß√£o do Projeto
Este projeto em Databricks realiza a ingest√£o de dados de duas fontes distintas, salvando os resultados como tabelas Delta em um cat√°logo e schema especificados. O pipeline √© projetado para ser robusto e seguro, utilizando as melhores pr√°ticas como o uso de Databricks Secrets e mecanismos de resili√™ncia.

As fontes de dados s√£o:

- **Banco de Dados SQL Server (AdventureWorks)**: Ingest√£o din√¢mica de todas as tabelas.
- **API RESTful**: Ingest√£o paralela e paginada de m√∫ltiplos endpoints.

## ‚ú® Principais Funcionalidades

- **Ingest√£o Din√¢mica do Banco**: O pipeline descobre e ingere automaticamente todas as tabelas do banco de dados de origem, sem a necessidade de list√°-las manualmente.
- **Robustez na Ingest√£o da API**:
  - *Retries com Limites Adaptativos*: Para cada endpoint da API, o c√≥digo realiza at√© 3 tentativas. Se uma requisi√ß√£o falha por timeout, o limite de registros (limit) √© reduzido progressivamente (de 100 para 50, e depois para 20) para aumentar a chance de sucesso em redes inst√°veis.
  - *Paralelismo*: As requisi√ß√µes para os diferentes endpoints da API s√£o feitas em paralelo usando ThreadPoolExecutor para otimizar o tempo de execu√ß√£o.
- **Sanitiza√ß√£o de Dados**: Os nomes das colunas vindas do banco de dados s√£o automaticamente sanitizados, substituindo espa√ßos por underscores (ex: `Product Name` se torna `Product_Name`).
- **Seguran√ßa**: Credenciais de acesso s√£o gerenciadas de forma segura atrav√©s do Databricks Secrets.
- **Estrutura Organizada**: Os dados s√£o salvos no Delta Lake dentro do cat√°logo `ted_dev` e schema `dev_guilherme_sobrinho`, com prefixos que identificam a origem (`raw_database_` ou `raw_api_`).

## ‚öôÔ∏è Tecnologias Utilizadas

- Apache Spark (PySpark)
- Delta Lake
- Databricks (Notebooks, Secrets, Repos)
- Python `concurrent.futures.ThreadPoolExecutor` para paraleliza√ß√£o.
- `requests` para consumo da API.

## üèóÔ∏è Arquitetura da Solu√ß√£o

### Banco de Dados SQL Server

- A conex√£o √© estabelecida via JDBC.
- O pipeline l√™ o cat√°logo `INFORMATION_SCHEMA.TABLES` para obter a lista de todas as tabelas do banco.
- Cada tabela √© lida e suas colunas s√£o sanitizadas antes de serem salvas.
- As tabelas s√£o salvas com o prefixo `raw_database_`.

### API RESTful

- S√£o ingeridos 4 endpoints espec√≠ficos: `SalesOrderDetail`, `SalesOrderHeader`, `PurchaseOrderDetail` e `PurchaseOrderHeader`.
- As requisi√ß√µes s√£o paginadas usando os par√¢metros `offset` e `limit`.
- A autentica√ß√£o √© feita via Basic Auth (usu√°rio e senha).
- A ingest√£o de m√∫ltiplos endpoints √© paralelizada com 2 workers por padr√£o.

## üóÉÔ∏è Esquema de Destino

- **Cat√°logo**: `ted_dev`
- **Schema**: `dev_guilherme_sobrinho`

**Nomenclatura das Tabelas**:
- `raw_database_<nome_tabela_sql>`
- `raw_api_<nome_endpoint_api>`

## üöÄ Como Executar o Pipeline

### 1. Clonar o Reposit√≥rio no Databricks

1. No menu lateral, v√° em **Repos**.
2. Clique em **Add Repo**.
3. Insira a URL do reposit√≥rio Git e confirme.

### 2. Configurar os Secrets

Crie um Secret Scope no Databricks com o nome `guilherme_ferreira_checkpoint2_lh`.

#### Via CLI do Databricks (Recomendado):

```bash
# Criar o scope
databricks secrets create-scope --scope guilherme_ferreira_checkpoint2_lh

# Adicionar secrets para o banco de dados
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh db_user --string-value "REDACTED"
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh db_pass --string-value "REDACTED"
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh db_host --string-value "REDACTED"
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh db_port --string-value "REDACTED" 

# Adicionar secrets para a API
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh api_user --string-value "REDACTED"
databricks secrets put-secret guilherme_ferreira_checkpoint2_lh api_password --string-value "REDACTED"
```

### 3. Executar o Notebook

1. Abra o notebook `databricks-database-ingestion`.
2. Anexe um cluster que tenha a depend√™ncia JDBC instalada (ver se√ß√£o abaixo).
3. Execute todas as c√©lulas do notebook (**Run All**). O pipeline far√° a ingest√£o completa dos dados do banco e da API.

## üì¶ Depend√™ncias

### Cluster Databricks

O cluster utilizado para executar o notebook deve ter a seguinte biblioteca Maven instalada:

- **Driver JDBC SQL Server**: `com.microsoft.sqlserver:mssql-jdbc:10.2.1.jre8`

### Bibliotecas Python

O c√≥digo utiliza as seguintes bibliotecas, que geralmente j√° v√™m pr√©-instaladas nos runtimes do Databricks:

- `pyspark`
- `requests`
- `json`
- `time`
- `datetime`
- `logging`
- `concurrent.futures`

## ‚ùå Opcional: Resetar o Ambiente

Caso queira apagar todas as tabelas ingeridas por este pipeline para uma reexecu√ß√£o limpa, voc√™ pode descomentar e executar a √∫ltima c√©lula do notebook. O c√≥digo ir√° listar e apagar todas as tabelas dentro do schema `ted_dev.dev_guilherme_sobrinho`.

```python
schema = "dev_guilherme_sobrinho"

# Listar todas as tabelas do schema
tabelas = spark.sql(f"SHOW TABLES IN {schema}").filter("isTemporary = false").select("tableName").collect()

for t in tabelas:
    nome_tabela = t["tableName"]
    full_table_name = f"{schema}.{nome_tabela}"
    try:
        logger.info(f"Apagando tabela {full_table_name} ...")
        spark.sql(f"DROP TABLE IF EXISTS {full_table_name}")
        logger.info(f"Tabela {full_table_name} apagada.")
    except Exception as e:
        logger.error(f"Erro ao apagar {full_table_name}: {e}")

```
